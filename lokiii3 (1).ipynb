{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 112281,
          "databundleVersionId": 13445283,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "lokiii3"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "PET3iLs4YWn2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "track1_india_al_impact_gen_ai_hackathon_path = kagglehub.competition_download('track1-india-al-impact-gen-ai-hackathon')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "1O6GcykXYWn_"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "# Ignore specific RuntimeWarning from rioxarray\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# Install required libraries\n",
        "!pip install --quiet rasterio terratorch\n",
        "\n",
        "# Import all necessary modules\n",
        "import os\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import lightning.pytorch as pl\n",
        "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
        "from terratorch.tasks import SemanticSegmentationTask\n",
        "\n",
        "# Define the base path to the dataset\n",
        "# We use the 'archive' path as it's cleaner\n",
        "BASE_DIR = \"/kaggle/input/track1-india-al-impact-gen-ai-hackathon/archive\"\n",
        "\n",
        "print(f\"Using base directory: {BASE_DIR}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T18:27:32.665627Z",
          "iopub.execute_input": "2025-09-16T18:27:32.665931Z",
          "iopub.status.idle": "2025-09-16T18:31:31.665227Z",
          "shell.execute_reply.started": "2025-09-16T18:27:32.665909Z",
          "shell.execute_reply": "2025-09-16T18:31:31.664145Z"
        },
        "id": "6QjJsu3H3B2m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet lightning"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T18:32:21.552271Z",
          "iopub.execute_input": "2025-09-16T18:32:21.553445Z",
          "iopub.status.idle": "2025-09-16T18:32:25.304112Z",
          "shell.execute_reply.started": "2025-09-16T18:32:21.553413Z",
          "shell.execute_reply": "2025-09-16T18:32:25.303106Z"
        },
        "id": "SAT1XUJW3B2m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import rasterio # Import rasterio\n",
        "import numpy as np # Import numpy\n",
        "import matplotlib.pyplot as plt # Import matplotlib.pyplot\n",
        "\n",
        "# Load Image IDs from the split files\n",
        "# The split files are located in the main directory after extraction\n",
        "# Using the standard Kaggle input path for competition data\n",
        "# The structure is typically /kaggle/input/competition-name/archive/...\n",
        "KAGGLE_INPUT_BASE = \"/kaggle/input/track1-india-al-impact-gen-ai-hackathon\"\n",
        "# Adjusted SPLIT_DIR path - assuming split files are under 'archive'\n",
        "# IMPORTANT: Please verify this path in your Kaggle notebook's input data section\n",
        "# and adjust SPLIT_DIR if the train.txt file is located elsewhere.\n",
        "SPLIT_DIR = os.path.join(KAGGLE_INPUT_BASE, 'archive') # Adjusted path to include 'archive'\n",
        "BASE_DIR = SPLIT_DIR # BASE_DIR is the same as SPLIT_DIR\n",
        "\n",
        "\n",
        "with open(os.path.join(SPLIT_DIR, \"train.txt\"), \"r\") as f:\n",
        "    train_ids = [line.strip() for line in f]\n",
        "\n",
        "print(f\"Found {len(train_ids)} training image IDs.\")\n",
        "print(f\"First 3 IDs: {train_ids[:3]}\")\n",
        "\n",
        "# --- Visualize a Sample ---\n",
        "sample_id = train_ids[0]\n",
        "\n",
        "input_path = os.path.join(BASE_DIR, \"train/inputs\", f\"{sample_id}_input.tif\")\n",
        "label_path = os.path.join(BASE_DIR, \"train/labels\", f\"{sample_id}_label_c6.tif\")\n",
        "\n",
        "print(f\"\\nVisualizing input: {input_path}\")\n",
        "print(f\"Visualizing label: {label_path}\")\n",
        "\n",
        "# Load images using rasterio\n",
        "with rasterio.open(input_path) as src:\n",
        "    # Read the first 3 bands (Red, Green, Blue) for visualization\n",
        "    # Check if there are enough bands for RGB, otherwise use the first band\n",
        "    num_bands = src.count\n",
        "    if num_bands >= 3:\n",
        "        image = src.read([1, 2, 3]).transpose(1, 2, 0)\n",
        "        # Normalize pixel values to be between 0 and 1 for proper display\n",
        "        image = (image - image.min()) / (image.max() - image.min())\n",
        "    else:\n",
        "        # If not enough bands for RGB, just read and display the first band in grayscale\n",
        "        image = src.read(1)\n",
        "        # Normalize the single band image\n",
        "        image = (image - image.min()) / (image.max() - image.min())\n",
        "        # Add a dummy dimension to make it compatible with imshow for grayscale\n",
        "        image = image[..., np.newaxis]\n",
        "\n",
        "\n",
        "with rasterio.open(label_path) as src:\n",
        "    label = src.read(1)\n",
        "\n",
        "# Display the images side-by-side\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "# For single band grayscale images, imshow might need cmap='gray'\n",
        "ax[0].imshow(image.squeeze(), cmap='gray' if image.shape[-1] == 1 else None)\n",
        "ax[0].set_title(f\"Input: {sample_id}\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(label, cmap='jet')\n",
        "ax[1].set_title(f\"Label Mask: {sample_id}\")\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T18:37:19.406047Z",
          "iopub.execute_input": "2025-09-16T18:37:19.406741Z",
          "iopub.status.idle": "2025-09-16T18:37:19.996344Z",
          "shell.execute_reply.started": "2025-09-16T18:37:19.406715Z",
          "shell.execute_reply": "2025-09-16T18:37:19.995513Z"
        },
        "id": "p4-4DHbg3B2n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# These are pre-computed statistics for the 12 bands in the dataset\n",
        "MEANS = [43.37, 38.76, 37.58, 39.39, 42.61, 54.78, 63.25, 59.99, 13.36, 69.21, 48.32, 69.70]\n",
        "STDS = [3.33, 4.16, 5.43, 9.23, 8.01, 6.74, 8.07, 7.84, 2.56, 16.96, 15.58, 9.25]\n",
        "\n",
        "datamodule = GenericNonGeoSegmentationDataModule(\n",
        "    batch_size=8,\n",
        "    num_workers=2,\n",
        "    num_classes=6,\n",
        "    train_data_root=os.path.join(BASE_DIR, 'train/inputs'),\n",
        "    train_label_data_root=os.path.join(BASE_DIR, 'train/labels'),\n",
        "    val_data_root=os.path.join(BASE_DIR, 'val/inputs'),\n",
        "    val_label_data_root=os.path.join(BASE_DIR, 'val/labels'),\n",
        "    test_data_root=os.path.join(BASE_DIR, 'test/inputs'),\n",
        "    predict_data_root=os.path.join(BASE_DIR, 'test/inputs'),\n",
        "    train_split=os.path.join(BASE_DIR, \"train.txt\"),\n",
        "    val_split=os.path.join(BASE_DIR, \"val.txt\"),\n",
        "    test_split=os.path.join(BASE_DIR, \"test.txt\"),\n",
        "    img_grep='*_input.tif',\n",
        "    label_grep='*_label_c6.tif',\n",
        "    means=MEANS,\n",
        "    stds=STDS,\n",
        ")\n",
        "\n",
        "datamodule.setup(\"fit\")\n",
        "print(f\"Train set size: {len(datamodule.train_dataset)}\")\n",
        "print(f\"Validation set size: {len(datamodule.val_dataset)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "fPvq6SvF3B2n",
        "execution": {
          "iopub.status.busy": "2025-09-16T18:37:28.028226Z",
          "iopub.execute_input": "2025-09-16T18:37:28.028845Z",
          "iopub.status.idle": "2025-09-16T18:37:28.139522Z",
          "shell.execute_reply.started": "2025-09-16T18:37:28.02882Z",
          "shell.execute_reply": "2025-09-16T18:37:28.138841Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Updated Datamodule and Trainer Configuration for Memory Management ---\n",
        "import lightning.pytorch as pl\n",
        "from terratorch.datamodules import GenericNonGeoSegmentationDataModule\n",
        "from terratorch.tasks import SemanticSegmentationTask\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchmetrics.classification import MulticlassJaccardIndex # Import MulticlassJaccardIndex\n",
        "# Removed problematic Dice import and torchvision transforms\n",
        "from lightning.pytorch.callbacks import LearningRateMonitor, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# These variables should be defined from your previous cells\n",
        "# Re-define them here for clarity if you are running this cell separately\n",
        "# Access the variable from the user namespace\n",
        "# Removed Colab-specific path access\n",
        "# track1_india_al_impact_gen_ai_hackathon_path = get_ipython().user_ns.get('track1_india_al_impact_gen_ai_hackathon_path')\n",
        "\n",
        "# Define BASE_DIR using the standard Kaggle input path\n",
        "# IMPORTANT: Verify this path in your Kaggle notebook's input data section\n",
        "KAGGLE_INPUT_BASE = \"/kaggle/input/track1-india-al-impact-gen-ai-hackathon\"\n",
        "# Assuming data and split files are under 'archive' as in the visualization cell\n",
        "BASE_DIR = os.path.join(KAGGLE_INPUT_BASE, 'archive')\n",
        "\n",
        "MEANS = [43.37, 38.76, 37.58, 39.39, 42.61, 54.78, 63.25, 59.99, 13.36, 69.21, 48.32, 69.70]\n",
        "STDS = [3.33, 4.16, 5.43, 9.23, 8.01, 6.74, 8.07, 7.84, 2.56, 16.96, 15.58, 9.25]\n",
        "CLASS_WEIGHTS = [0.4761, 0.1702, 0.0334, 0.2674, 0.0204, 0.0326]\n",
        "model_args = {\n",
        "    \"backbone\": \"prithvi_eo_v2_300_tl\",\n",
        "    \"backbone_pretrained\": True,\n",
        "    \"backbone_bands\": list(range(1, 13)),\n",
        "    \"decoder\": \"UperNetDecoder\",\n",
        "    \"num_classes\": 6,\n",
        "}\n",
        "# --- UPDATED BATCH SIZE ---\n",
        "# Reduce the batch size to a smaller value to fit in memory\n",
        "# A value of 2 is a good starting point.\n",
        "BATCH_SIZE = 2 # Further reduced batch size\n",
        "\n",
        "# --- Removed Data Augmentation Transformations ---\n",
        "train_transform = None\n",
        "val_test_transform = None\n",
        "predict_transform = None # Set predict_transform to None\n",
        "\n",
        "\n",
        "# The datamodule configuration is correct.\n",
        "datamodule = GenericNonGeoSegmentationDataModule(\n",
        "    batch_size=BATCH_SIZE, # <--- UPDATED: Smaller batch size\n",
        "    num_workers=2,\n",
        "    num_classes=6,\n",
        "    train_data_root=os.path.join(BASE_DIR, 'train/inputs'),\n",
        "    train_label_data_root=os.path.join(BASE_DIR, 'train/labels'),\n",
        "    val_data_root=os.path.join(BASE_DIR, 'val/inputs'),\n",
        "    val_label_data_root=os.path.join(BASE_DIR, 'val/labels'),\n",
        "    test_data_root=os.path.join(BASE_DIR, 'val/inputs'),\n",
        "    test_label_data_root=os.path.join(BASE_DIR, 'val/labels'),\n",
        "    predict_data_root=os.path.join(BASE_DIR, 'test/inputs'),\n",
        "    train_split=os.path.join(BASE_DIR, \"train.txt\"), # Use BASE_DIR for split files\n",
        "    val_split=os.path.join(BASE_DIR, \"val.txt\"),   # Use BASE_DIR for split files\n",
        "    test_split=os.path.join(BASE_DIR, \"test.txt\"), # Use BASE_DIR for split files\n",
        "    img_grep='*_input.tif',\n",
        "    label_grep='*_label_c6.tif',\n",
        "    means=MEANS,\n",
        "    stds=STDS,\n",
        "    train_transform=train_transform, # Removed training transformations\n",
        "    val_transform=val_test_transform, # No validation transformations\n",
        "    test_transform=val_test_transform, # No test transformations\n",
        "    predict_transform=predict_transform # No predict transformations\n",
        ")\n",
        "datamodule.setup(\"fit\")\n",
        "\n",
        "\n",
        "# --- Define Loss Function (Using Cross-Entropy) ---\n",
        "# Removed CombinedLoss with Dice due to import issues\n",
        "# loss_fn = nn.CrossEntropyLoss(ignore_index=-1) # Removed direct instantiation\n",
        "\n",
        "\n",
        "# The model definition is updated with a lower learning rate and Cross-Entropy loss.\n",
        "model = SemanticSegmentationTask(\n",
        "    model_args=model_args,\n",
        "    model_factory=\"EncoderDecoderFactory\",\n",
        "    class_weights=CLASS_WEIGHTS, # Class weights can be used within the loss function if it supports it\n",
        "    loss=\"ce\", # Use Cross-Entropy loss with the string key\n",
        "    lr=1e-5, # Increased learning rate slightly from 5e-7 to 1e-5\n",
        "    optimizer=\"AdamW\",\n",
        "    freeze_backbone=False,\n",
        "    ignore_index=-1 # Ignore index passed to the loss function and datamodule\n",
        ")\n",
        "\n",
        "# Configure the Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=100, # Increased max epochs to 100\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    logger=pl.loggers.TensorBoardLogger(save_dir=\"logs/\", name=\"semantic_segmentation\"),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor=\"val/loss\", mode=\"min\", patience=10), # Increased patience\n",
        "        ModelCheckpoint(\n",
        "            dirpath=\"checkpoints/\",\n",
        "            monitor=\"val/Multiclass_Jaccard_Index\", # Monitoring Jaccard Index\n",
        "            mode=\"max\",\n",
        "            save_top_k=1,\n",
        "            filename=\"best-model-{epoch:02d}-{val/Multiclass_Jaccard_Index:.4f}\",\n",
        "            save_last=True\n",
        "        ),\n",
        "         # Add learning rate monitor callback (optional, but good for debugging)\n",
        "        LearningRateMonitor(logging_interval='step'),\n",
        "        # Removed ReduceLROnPlateau callback due to import issues\n",
        "    ],\n",
        "    log_every_n_steps=10,\n",
        "    gradient_clip_val=1.0\n",
        ")\n",
        "\n",
        "print(\"Starting model training with updated configuration...\")\n",
        "trainer.fit(model, datamodule=datamodule)\n",
        "print(\"Training finished!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "bWf50EWl3B2n",
        "execution": {
          "iopub.status.busy": "2025-09-16T19:34:40.906468Z",
          "iopub.execute_input": "2025-09-16T19:34:40.906813Z",
          "iopub.status.idle": "2025-09-16T19:40:45.394543Z",
          "shell.execute_reply.started": "2025-09-16T19:34:40.906785Z",
          "shell.execute_reply": "2025-09-16T19:40:45.390758Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics.classification\n",
        "\n",
        "print(dir(torchmetrics.classification))"
      ],
      "metadata": {
        "id": "a309d1be",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Validation Step ---\n",
        "print(\"Starting validation...\")\n",
        "validation_results = trainer.validate(model, datamodule=datamodule)\n",
        "print(\"Validation results:\", validation_results)\n",
        "\n",
        "# Print detailed metrics\n",
        "if validation_results:\n",
        "    print(\"\\nDetailed Validation Metrics:\")\n",
        "    for metric_name, metric_value in validation_results[0].items():\n",
        "        print(f\"{metric_name}: {metric_value:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "TqrhjY0R3B2o",
        "execution": {
          "iopub.status.busy": "2025-09-16T19:06:17.553767Z",
          "iopub.execute_input": "2025-09-16T19:06:17.554498Z",
          "iopub.status.idle": "2025-09-16T19:06:33.345409Z",
          "shell.execute_reply.started": "2025-09-16T19:06:17.554472Z",
          "shell.execute_reply": "2025-09-16T19:06:33.344588Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Step ---\n",
        "print(\"Starting testing...\")\n",
        "test_results = trainer.test(model, datamodule=datamodule)\n",
        "print(\"Test results:\", test_results)\n",
        "\n",
        "# Print detailed test metrics\n",
        "if test_results:\n",
        "    print(\"\\nDetailed Test Metrics:\")\n",
        "    for metric_name, metric_value in test_results[0].items():\n",
        "        print(f\"{metric_name}: {metric_value:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "d-87E2b33B2o",
        "execution": {
          "iopub.status.busy": "2025-09-16T19:06:40.594072Z",
          "iopub.execute_input": "2025-09-16T19:06:40.594859Z",
          "iopub.status.idle": "2025-09-16T19:06:44.542612Z",
          "shell.execute_reply.started": "2025-09-16T19:06:40.59483Z",
          "shell.execute_reply": "2025-09-16T19:06:44.541544Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Debug: Check Prediction Output Structure ---\n",
        "import torch\n",
        "\n",
        "print(\"Debugging prediction output structure...\")\n",
        "\n",
        "# Set up the datamodule for the prediction stage\n",
        "datamodule.setup(\"predict\")\n",
        "predict_dataloader = datamodule.predict_dataloader()\n",
        "\n",
        "# Get one batch to inspect the output structure\n",
        "test_batch = next(iter(predict_dataloader))\n",
        "\n",
        "# The model expects a tensor, not a dictionary.\n",
        "test_batch_image = test_batch['image']\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    # Pass the extracted tensor to the model\n",
        "    output = model(test_batch_image)\n",
        "\n",
        "print(f\"Output type: {type(output)}\")\n",
        "\n",
        "# Recursively check for tensors, including nested dictionaries and lists\n",
        "def inspect_structure(obj, depth=0, path=\"\"):\n",
        "    indent = \"  \" * depth\n",
        "    if isinstance(obj, torch.Tensor):\n",
        "        print(f\"{indent}{path}: Tensor(shape={obj.shape}, dtype={obj.dtype})\")\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        print(f\"{indent}{path}: {type(obj)._name_} with {len(obj)} elements\")\n",
        "        for i, item in enumerate(obj):\n",
        "            inspect_structure(item, depth + 1, f\"{path}[{i}]\")\n",
        "    elif isinstance(obj, dict):\n",
        "        print(f\"{indent}{path}: dict with {len(obj)} keys\")\n",
        "        for key, value in obj.items():\n",
        "            inspect_structure(value, depth + 1, f\"{path}['{key}']\")\n",
        "    else:\n",
        "        print(f\"{indent}{path}: {type(obj)}\")\n",
        "\n",
        "# The model's output is an object of type ModelOutput, but it behaves like a dictionary\n",
        "# We can inspect its contents directly.\n",
        "inspect_structure(output, path=\"output\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "PU0tcsvA3B2o",
        "execution": {
          "iopub.status.busy": "2025-09-16T19:11:50.373381Z",
          "iopub.execute_input": "2025-09-16T19:11:50.373753Z",
          "iopub.status.idle": "2025-09-16T19:12:00.792322Z",
          "shell.execute_reply.started": "2025-09-16T19:11:50.373724Z",
          "shell.execute_reply": "2025-09-16T19:12:00.791157Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to get prediction\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, imgs, masks = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)             # raw logits\n",
        "            predictions = torch.argmax(outputs, dim=1)  # convert to class indices\n",
        "\n",
        "            preds.append(predictions.cpu())\n",
        "            imgs.append(images.cpu())\n",
        "            masks.append(targets.cpu())\n",
        "    return imgs, masks, preds\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "f1sJOByi3B2p",
        "execution": {
          "iopub.status.busy": "2025-09-16T19:12:07.085209Z",
          "iopub.execute_input": "2025-09-16T19:12:07.086244Z",
          "iopub.status.idle": "2025-09-16T19:12:07.091984Z",
          "shell.execute_reply.started": "2025-09-16T19:12:07.086201Z",
          "shell.execute_reply": "2025-09-16T19:12:07.090974Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to visualize input, ground truth and prediction\n",
        "def visualize_predictions(imgs, masks, preds, num_samples=3):\n",
        "    for i in range(num_samples):\n",
        "        plt.figure(figsize=(12,4))\n",
        "\n",
        "        # Input image (first channel or RGB if available)\n",
        "        img = imgs[i][0].permute(1,2,0).numpy() if imgs[i][0].shape[0] > 1 else imgs[i][0][0].numpy()\n",
        "        plt.subplot(1,3,1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Ground Truth Mask\n",
        "        plt.subplot(1,3,2)\n",
        "        plt.imshow(masks[i][0], cmap=\"tab20\")\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Predicted Mask\n",
        "        plt.subplot(1,3,3)\n",
        "        plt.imshow(preds[i][0], cmap=\"tab20\")\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "FoTDaAoW3B2p",
        "execution": {
          "iopub.status.busy": "2025-09-16T19:12:15.076673Z",
          "iopub.execute_input": "2025-09-16T19:12:15.077357Z",
          "iopub.status.idle": "2025-09-16T19:12:15.084097Z",
          "shell.execute_reply.started": "2025-09-16T19:12:15.077329Z",
          "shell.execute_reply": "2025-09-16T19:12:15.083138Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Generate a `prediction.csv` file containing predictions on the test dataset using the trained model."
      ],
      "metadata": {
        "id": "7a71a8cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions\n",
        "\n",
        "### Subtask:\n",
        "Use the trained model to generate predictions on the test dataset.\n"
      ],
      "metadata": {
        "id": "4f6233a1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Use the trained model to generate predictions on the test dataset by calling the previously defined `predict` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "ad0a052e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to get prediction - UPDATED to handle dictionary output\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, imgs, masks, filenames = [], [], [], [] # Include filenames to store\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader: # Iterate over batches (which are dictionaries)\n",
        "            images = batch['image'].to(device) # Extract images from the dictionary\n",
        "            targets = batch['mask'].to(device)  # Extract masks from the dictionary\n",
        "            filenames.extend(batch['filename']) # Extend the filenames list\n",
        "\n",
        "            outputs = model(images)             # raw logits\n",
        "            predictions = torch.argmax(outputs, dim=1)  # convert to class indices\n",
        "\n",
        "            preds.append(predictions.cpu())\n",
        "            imgs.append(images.cpu())\n",
        "            masks.append(targets.cpu())\n",
        "\n",
        "    # Concatenate the lists of tensors into single tensors\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    masks = torch.cat(masks, dim=0)\n",
        "\n",
        "    return imgs, masks, preds, filenames # Return filenames as well"
      ],
      "metadata": {
        "id": "TyjCrKMS8ula",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:22:36.269847Z",
          "iopub.execute_input": "2025-09-16T19:22:36.270172Z",
          "iopub.status.idle": "2025-09-16T19:22:36.277106Z",
          "shell.execute_reply.started": "2025-09-16T19:22:36.27015Z",
          "shell.execute_reply": "2025-09-16T19:22:36.276219Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to access `.logits` or `.pred` attributes failed, and the printed attributes of the `ModelOutput` object include `output`. This suggests that the actual tensor containing the raw logits is likely stored in the `.output` attribute. Let's update the `predict` function to access `outputs.output` to retrieve the tensor before applying `argmax`.\n",
        "\n"
      ],
      "metadata": {
        "id": "ec88bf1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to get prediction - UPDATED to access .output attribute\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, imgs, masks, filenames = [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch['image'].to(device)\n",
        "            targets = batch['mask'].to(device)\n",
        "            filenames.extend(batch['filename'])\n",
        "\n",
        "            outputs = model(images)             # outputs is now a ModelOutput object\n",
        "\n",
        "            # Access the .output attribute which likely contains the logits tensor\n",
        "            try:\n",
        "                logits = outputs.output\n",
        "            except AttributeError:\n",
        "                print(f\"Could not find .output attribute in ModelOutput.\")\n",
        "                print(f\"ModelOutput attributes: {dir(outputs)}\")\n",
        "                # As a last resort, attempt to use the object directly (which we know failed)\n",
        "                logits = outputs\n",
        "\n",
        "\n",
        "            predictions = torch.argmax(logits, dim=1)  # convert to class indices\n",
        "\n",
        "            preds.append(predictions.cpu())\n",
        "            imgs.append(images.cpu())\n",
        "            masks.append(targets.cpu())\n",
        "\n",
        "    # Concatenate the lists of tensors into single tensors\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    masks = torch.cat(masks, dim=0)\n",
        "\n",
        "    return imgs, masks, preds, filenames\n",
        "\n",
        "# Determine the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Set the datamodule for the predict stage and get the dataloader\n",
        "datamodule.setup(\"predict\")\n",
        "predict_dataloader = datamodule.predict_dataloader()\n",
        "\n",
        "# Generate predictions using the updated predict function\n",
        "imgs, masks, preds, filenames = predict(model, predict_dataloader, device)\n",
        "\n",
        "print(f\"Generated predictions for {len(preds)} images.\")\n",
        "# Optionally, inspect shapes of the results\n",
        "if len(preds) > 0:\n",
        "    print(f\"Shape of images tensor: {imgs.shape}\")\n",
        "    print(f\"Shape of masks tensor: {masks.shape}\")\n",
        "    print(f\"Shape of predictions tensor: {preds.shape}\")\n",
        "    print(f\"Number of filenames: {len(filenames)}\")\n"
      ],
      "metadata": {
        "id": "1KBl_dpC8ztf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:22:49.892155Z",
          "iopub.execute_input": "2025-09-16T19:22:49.892754Z",
          "iopub.status.idle": "2025-09-16T19:23:04.761037Z",
          "shell.execute_reply.started": "2025-09-16T19:22:49.892728Z",
          "shell.execute_reply": "2025-09-16T19:23:04.759855Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format predictions\n",
        "\n",
        "### Subtask:\n",
        "Process the model's output to extract the predicted class labels for each pixel and format them according to the submission requirements (typically a run-length encoding or similar format for segmentation masks).\n"
      ],
      "metadata": {
        "id": "a3485de9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Define the RLE function and iterate through the predicted masks to generate RLE strings and store them with their filenames.\n",
        "\n"
      ],
      "metadata": {
        "id": "e50ef4f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform Run-Length Encoding (RLE)\n",
        "def rle_encode(mask):\n",
        "    \"\"\"\n",
        "    Encodes a 2D binary mask into RLE format.\n",
        "\n",
        "    Args:\n",
        "        mask (np.ndarray): A 2D numpy array representing the mask.\n",
        "\n",
        "    Returns:\n",
        "        str: The RLE encoded string.\n",
        "    \"\"\"\n",
        "    pixels = mask.flatten()\n",
        "    # We need to pad the array with -1 to ensure the last run is captured\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    # Only encode runs of the positive class (assuming class 1 is the target)\n",
        "    # For multi-class, we need to encode each class separately or adapt the format.\n",
        "    # Based on typical Kaggle segmentation, we'll encode the non-zero pixels.\n",
        "    # Let's assume we are encoding a single class for submission purposes.\n",
        "    # If the submission requires encoding all classes, this function would need modification.\n",
        "    # For this task, we'll encode based on the predicted class value.\n",
        "    # The RLE format is typically 'pixel_start run_length pixel_start run_length ...'\n",
        "\n",
        "    # Let's adapt to encode runs of identical values, including 0.\n",
        "    # This might be more general for multi-class segmentation if the format allows.\n",
        "    # A common format is 'start length' for positive class pixels.\n",
        "    # Let's stick to the common format for now, assuming we need to encode each class separately later if needed.\n",
        "    # For this step, we will just generate RLE for the raw predicted values.\n",
        "\n",
        "    # A more standard RLE for segmentation would encode the starting pixel index (1-based) and the run length.\n",
        "    # Let's refine the RLE to match a typical submission format: space-separated pairs of 'start length'.\n",
        "    # The mask is flattened row by row.\n",
        "    flat_mask = mask.flatten()\n",
        "    rle = []\n",
        "    last_value = -1\n",
        "    current_start = 0\n",
        "    for i, value in enumerate(flat_mask):\n",
        "        if value != last_value:\n",
        "            if last_value != -1: # End of a run\n",
        "                 # If we were tracking a specific class (e.g., non-zero), we'd add here\n",
        "                 pass # We will generate RLE for all values for now\n",
        "\n",
        "            last_value = value\n",
        "            current_start = i + 1 # RLE is 1-based\n",
        "    # Handle the last run\n",
        "    # This simple RLE doesn't seem right for the typical Kaggle format.\n",
        "\n",
        "    # Let's try a common RLE implementation for binary masks, which we can adapt for multi-class.\n",
        "    # This RLE generates pairs of (start_pixel, length) for *positive* pixels.\n",
        "    # For multi-class, we'd need to generate RLE for each class.\n",
        "    # Given the task is to process the *model output* and format it, we'll generate RLE for each class present in the mask.\n",
        "\n",
        "    rle_strings = {}\n",
        "    unique_classes = np.unique(mask)\n",
        "\n",
        "    for class_id in unique_classes:\n",
        "        if class_id == 0: # Typically background is not encoded\n",
        "            continue\n",
        "\n",
        "        binary_mask = (mask == class_id).astype(np.uint8)\n",
        "        flat_mask = binary_mask.flatten()\n",
        "        pixels = np.concatenate([[0], flat_mask, [0]])\n",
        "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "        runs[1::2] -= runs[::2]\n",
        "\n",
        "        # The RLE format is 'start length' pairs, space separated.\n",
        "        # Only include runs where the value is 1 (the current class)\n",
        "        rle_list = []\n",
        "        for i in range(0, len(runs), 2):\n",
        "            start = runs[i]\n",
        "            length = runs[i+1]\n",
        "            # Check if the run corresponds to the current class (value 1 in the binary mask)\n",
        "            # We need to check the value of the pixel at 'start - 1' in the original flattened mask\n",
        "            if flat_mask[start - 1] == 1:\n",
        "                 rle_list.append(str(start))\n",
        "                 rle_list.append(str(length))\n",
        "\n",
        "        rle_strings[class_id] = \" \".join(rle_list)\n",
        "\n",
        "    # The submission format might require a single string per image,\n",
        "    # possibly with class IDs embedded or a separate entry per class per image.\n",
        "    # Let's generate a list of dictionaries, where each dict is {'ImageId': ..., 'ClassId': ..., 'EncodedPixels': ...}\n",
        "    # This seems more aligned with common multi-class segmentation formats.\n",
        "    # For this subtask, we just need to get the RLE strings.\n",
        "\n",
        "    # Let's return a dictionary mapping class_id to RLE string for non-zero classes.\n",
        "    return rle_strings\n",
        "\n",
        "\n",
        "# Iterate through predictions and generate RLE\n",
        "rle_data = []\n",
        "for i in range(len(preds)):\n",
        "    image_id_full = filenames[i]\n",
        "    # Extract the base image ID (e.g., 'agrifieldnet_195f2')\n",
        "    # Assuming filename format is something like '/path/to/agrifieldnet_195f2_input.tif'\n",
        "    image_id = os.path.basename(image_id_full).replace('_input.tif', '')\n",
        "\n",
        "    predicted_mask_tensor = preds[i]\n",
        "    predicted_mask_np = predicted_mask_tensor.squeeze().numpy() # Remove batch dimension and convert to numpy\n",
        "\n",
        "    # Generate RLE for each class in the predicted mask\n",
        "    rle_strings_by_class = rle_encode(predicted_mask_np)\n",
        "\n",
        "    # Store the RLE data. Each entry in rle_data will represent an image and its RLEs per class.\n",
        "    # We'll store it as a dictionary for now, mapping class ID to RLE string.\n",
        "    # The final submission format will require restructuring this.\n",
        "    rle_data.append({'ImageId': image_id, 'RLEs': rle_strings_by_class})\n",
        "\n",
        "print(f\"Generated RLE data for {len(rle_data)} images.\")\n",
        "# Optionally, print a sample\n",
        "if len(rle_data) > 0:\n",
        "    print(\"\\nSample RLE data for the first image:\")\n",
        "    print(rle_data[0])"
      ],
      "metadata": {
        "id": "c7f2dd4d",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:23:12.601284Z",
          "iopub.execute_input": "2025-09-16T19:23:12.601868Z",
          "iopub.status.idle": "2025-09-16T19:23:13.543384Z",
          "shell.execute_reply.started": "2025-09-16T19:23:12.601834Z",
          "shell.execute_reply": "2025-09-16T19:23:13.542559Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create submission file\n",
        "\n",
        "### Subtask:\n",
        "Generate a CSV file containing the image IDs from the test set and their corresponding formatted predictions.\n"
      ],
      "metadata": {
        "id": "a3b083e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Create a list of rows for the submission DataFrame by iterating through the generated RLE data and formatting it as required for the submission file. Then create a pandas DataFrame from this list and display its head.\n",
        "\n"
      ],
      "metadata": {
        "id": "49f8bfd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store the rows for the submission DataFrame\n",
        "submission_rows = []\n",
        "\n",
        "# Iterate through the rle_data list\n",
        "for image_data in rle_data:\n",
        "    image_id = image_data['ImageId']\n",
        "    rles_by_class = image_data['RLEs']\n",
        "\n",
        "    # Iterate through the 'RLEs' dictionary for each image\n",
        "    for class_id, rle_string in rles_by_class.items():\n",
        "        # Create a dictionary for each row\n",
        "        row = {\n",
        "            'ImageId': image_id,\n",
        "            'ClassId': class_id,\n",
        "            'EncodedPixels': rle_string\n",
        "        }\n",
        "        # Append the dictionary to the list of rows\n",
        "        submission_rows.append(row)\n",
        "\n",
        "# Create a pandas DataFrame from the list of rows\n",
        "submission_df = pd.DataFrame(submission_rows)\n",
        "\n",
        "# Print the head of the created DataFrame\n",
        "print(\"Submission DataFrame Head:\")\n",
        "display(submission_df.head())"
      ],
      "metadata": {
        "id": "67223e64",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:23:20.94454Z",
          "iopub.execute_input": "2025-09-16T19:23:20.944859Z",
          "iopub.status.idle": "2025-09-16T19:23:21.037366Z",
          "shell.execute_reply.started": "2025-09-16T19:23:20.944837Z",
          "shell.execute_reply": "2025-09-16T19:23:21.036595Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "The submission DataFrame is empty, which indicates that the `rle_data` list might be empty or the `RLEs` dictionary within each entry is empty. This could be due to the model predicting only the background class (class 0), which is typically not encoded in RLE for submission. To address this, I need to ensure that even if no foreground classes are predicted, an entry for the image with an empty 'EncodedPixels' is included for each expected class ID (1 to 5, as class 0 is background). I will modify the loop to iterate through the expected class IDs (1 to 5) for each image and add a row with an empty RLE string if no RLE was generated for that class.\n",
        "\n"
      ],
      "metadata": {
        "id": "8f90da9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an empty list to store the rows for the submission DataFrame\n",
        "submission_rows = []\n",
        "\n",
        "# Define the expected class IDs for the submission (assuming 1 to 5, excluding background 0)\n",
        "expected_class_ids = list(range(1, 6)) # Classes 1 through 5\n",
        "\n",
        "# Iterate through the rle_data list\n",
        "for image_data in rle_data:\n",
        "    image_id = image_data['ImageId']\n",
        "    rles_by_class = image_data['RLEs']\n",
        "\n",
        "    # Iterate through the expected class IDs\n",
        "    for class_id in expected_class_ids:\n",
        "        # Get the RLE string for the current class, or an empty string if not present\n",
        "        rle_string = rles_by_class.get(class_id, \"\")\n",
        "\n",
        "        # Create a dictionary for each row\n",
        "        row = {\n",
        "            'ImageId': image_id,\n",
        "            'ClassId': class_id,\n",
        "            'EncodedPixels': rle_string\n",
        "        }\n",
        "        # Append the dictionary to the list of rows\n",
        "        submission_rows.append(row)\n",
        "\n",
        "# Create a pandas DataFrame from the list of rows\n",
        "submission_df = pd.DataFrame(submission_rows)\n",
        "\n",
        "# Print the head of the created DataFrame\n",
        "print(\"Submission DataFrame Head:\")\n",
        "display(submission_df.head())\n",
        "\n",
        "# Print the shape of the DataFrame to confirm the number of rows\n",
        "print(f\"\\nSubmission DataFrame shape: {submission_df.shape}\")\n",
        "\n",
        "# Optionally, check for images with no predictions for any class\n",
        "images_with_predictions = submission_df[submission_df['EncodedPixels'] != '']['ImageId'].unique()\n",
        "all_image_ids = [data['ImageId'] for data in rle_data]\n",
        "images_with_no_foreground_predictions = [img_id for img_id in all_image_ids if img_id not in images_with_predictions]\n",
        "\n",
        "print(f\"\\nTotal unique images in test set: {len(all_image_ids)}\")\n",
        "print(f\"Images with at least one foreground prediction: {len(images_with_predictions)}\")\n",
        "print(f\"Images with no foreground predictions: {len(images_with_no_foreground_predictions)}\")"
      ],
      "metadata": {
        "id": "99cfa236",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:23:30.610045Z",
          "iopub.execute_input": "2025-09-16T19:23:30.610331Z",
          "iopub.status.idle": "2025-09-16T19:23:30.664496Z",
          "shell.execute_reply.started": "2025-09-16T19:23:30.610313Z",
          "shell.execute_reply": "2025-09-16T19:23:30.663769Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save submission file\n",
        "\n",
        "### Subtask:\n",
        "Save the generated CSV file to the desired location.\n"
      ],
      "metadata": {
        "id": "dc0dd80f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Save the generated submission DataFrame to a CSV file named 'submission.csv' without including the index.\n",
        "\n"
      ],
      "metadata": {
        "id": "a047760f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the submission DataFrame to a CSV file in the Kaggle working directory\n",
        "submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file '/kaggle/working/submission.csv' created successfully.\")"
      ],
      "metadata": {
        "id": "e8dd2f9a",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:27:27.830677Z",
          "iopub.execute_input": "2025-09-16T19:27:27.831063Z",
          "iopub.status.idle": "2025-09-16T19:27:27.838486Z",
          "shell.execute_reply.started": "2025-09-16T19:27:27.83104Z",
          "shell.execute_reply": "2025-09-16T19:27:27.837426Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `predict_dataloader` provides image, mask, and filename information within a dictionary structure.\n",
        "*   The trained model outputs a `ModelOutput` object, requiring access to the `.output` attribute to retrieve the raw logits tensor for prediction.\n",
        "*   Predictions were successfully generated for all images in the test set, resulting in a tensor of predicted class indices for each pixel.\n",
        "*   Run-Length Encoding (RLE) was applied to the predicted masks for each foreground class (classes 1 through 5), generating RLE strings in the 'start length' format.\n",
        "*   A submission DataFrame was created with columns 'ImageId', 'ClassId', and 'EncodedPixels', ensuring an entry for every image and every expected foreground class (1-5). An empty string is used for 'EncodedPixels' if no foreground pixels were predicted for a specific class in an image.\n",
        "*   In this specific execution, no foreground predictions were made for any image, resulting in all 'EncodedPixels' being empty strings in the submission DataFrame.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current model training might require further tuning or a different architecture as it appears to predict only the background class for the test set, resulting in empty RLE strings for all foreground classes.\n",
        "*   The generated `submission.csv` file, although correctly formatted, contains only empty predictions for foreground classes, indicating that the model's performance on the test set is currently very low for detecting non-background regions.\n"
      ],
      "metadata": {
        "id": "454159d3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Analyze the file \"/content/submission.csv\" to determine the percentage of pixels predicted for each of the following crop types: Gram, Maize, Mustard, Sugarcane, Wheat, and Other Crop."
      ],
      "metadata": {
        "id": "27626b13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the submission data\n",
        "\n",
        "### Subtask:\n",
        "Load the `submission.csv` file into a pandas DataFrame.\n"
      ],
      "metadata": {
        "id": "c5447307"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Load the submission.csv file into a pandas DataFrame and display the head.\n",
        "\n"
      ],
      "metadata": {
        "id": "002285ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the submission.csv file from the Kaggle working directory\n",
        "submission_df = pd.read_csv('/kaggle/working/submission.csv')\n",
        "display(submission_df.head())"
      ],
      "metadata": {
        "id": "8e386748",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:28:05.055716Z",
          "iopub.execute_input": "2025-09-16T19:28:05.056399Z",
          "iopub.status.idle": "2025-09-16T19:28:05.076693Z",
          "shell.execute_reply.started": "2025-09-16T19:28:05.056373Z",
          "shell.execute_reply": "2025-09-16T19:28:05.075966Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process rle and calculate pixel counts\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the DataFrame, decode the RLE strings for each predicted class in each image, and calculate the total number of pixels predicted for each crop type across all images.\n"
      ],
      "metadata": {
        "id": "f778c8ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Define the RLE decode function and initialize the pixel counts dictionary. Then iterate through the submission DataFrame, decode the RLE for each entry, and accumulate pixel counts for each class. I will assume the image dimensions are 256x256 based on the earlier visualization of a sample image.\n",
        "\n"
      ],
      "metadata": {
        "id": "4a1c932b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the rle_decode function\n",
        "def rle_decode(rle_string, height, width):\n",
        "    \"\"\"\n",
        "    Decodes an RLE encoded string into a binary mask.\n",
        "\n",
        "    Args:\n",
        "        rle_string (str): The RLE encoded string in 'start length' format,\n",
        "                          space separated.\n",
        "        height (int): The height of the mask.\n",
        "        width (int): The width of the mask.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D numpy array (height, width) representing the decoded mask.\n",
        "    \"\"\"\n",
        "    mask = np.zeros(height * width, dtype=np.uint8)\n",
        "    if rle_string:\n",
        "        rle_parts = list(map(int, rle_string.split()))\n",
        "        for i in range(0, len(rle_parts), 2):\n",
        "            start = rle_parts[i] - 1  # RLE is 1-based, numpy is 0-based\n",
        "            length = rle_parts[i+1]\n",
        "            end = start + length\n",
        "            mask[start:end] = 1\n",
        "    return mask.reshape(height, width)\n",
        "\n",
        "# Initialize a dictionary to store pixel counts for each crop type (ClassId 1-5)\n",
        "crop_pixel_counts = {class_id: 0 for class_id in range(1, 6)}\n",
        "\n",
        "# Define assumed image dimensions based on sample visualization\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "\n",
        "# Iterate through each row of the submission DataFrame\n",
        "for index, row in submission_df.iterrows():\n",
        "    image_id = row['ImageId']\n",
        "    class_id = row['ClassId']\n",
        "    rle_string = row['EncodedPixels']\n",
        "\n",
        "    # Ensure the class_id is one we are tracking (1-5)\n",
        "    if class_id in crop_pixel_counts:\n",
        "        # Decode the RLE string\n",
        "        # Handle potential NaN values in 'EncodedPixels' by treating them as empty strings\n",
        "        decoded_mask = rle_decode(str(rle_string) if pd.notna(rle_string) else \"\",\n",
        "                                  IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "\n",
        "        # Calculate the number of pixels predicted for the current class\n",
        "        pixel_count = np.sum(decoded_mask)\n",
        "\n",
        "        # Add the count to the corresponding crop type\n",
        "        crop_pixel_counts[class_id] += pixel_count\n",
        "\n",
        "print(\"Total pixel counts for each crop type:\")\n",
        "print(crop_pixel_counts)"
      ],
      "metadata": {
        "id": "6ca95276",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:28:28.51578Z",
          "iopub.execute_input": "2025-09-16T19:28:28.516116Z",
          "iopub.status.idle": "2025-09-16T19:28:28.612125Z",
          "shell.execute_reply.started": "2025-09-16T19:28:28.516081Z",
          "shell.execute_reply": "2025-09-16T19:28:28.611269Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get total image dimensions\n",
        "\n",
        "### Subtask:\n",
        "Obtain the dimensions of the images to calculate the total number of pixels in each image.\n"
      ],
      "metadata": {
        "id": "fc73ef19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Define variables IMAGE_HEIGHT and IMAGE_WIDTH based on the known dimensions of the input images.\n",
        "\n"
      ],
      "metadata": {
        "id": "77a9f711"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dimensions of the input images\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "\n",
        "print(f\"Image height: {IMAGE_HEIGHT}, Image width: {IMAGE_WIDTH}\")"
      ],
      "metadata": {
        "id": "5607acf6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:28:36.552439Z",
          "iopub.execute_input": "2025-09-16T19:28:36.553156Z",
          "iopub.status.idle": "2025-09-16T19:28:36.557498Z",
          "shell.execute_reply.started": "2025-09-16T19:28:36.55313Z",
          "shell.execute_reply": "2025-09-16T19:28:36.556695Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate total pixels across all images\n",
        "\n",
        "### Subtask:\n",
        "Sum the total number of pixels for all images in the test set.\n"
      ],
      "metadata": {
        "id": "1de6d041"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Calculate the total number of pixels across all images and store it in `total_pixels_all_images`, then print the result to complete the subtask.\n",
        "\n"
      ],
      "metadata": {
        "id": "c95975b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of pixels in a single image\n",
        "total_pixels_per_image = IMAGE_HEIGHT * IMAGE_WIDTH\n",
        "\n",
        "# Determine the total number of unique images in the test set\n",
        "num_unique_images = submission_df['ImageId'].nunique()\n",
        "\n",
        "# Calculate the total number of pixels across all images\n",
        "total_pixels_all_images = total_pixels_per_image * num_unique_images\n",
        "\n",
        "# Print the total number of pixels across all images\n",
        "print(f\"Total number of pixels across all images: {total_pixels_all_images}\")"
      ],
      "metadata": {
        "id": "c0e030cf",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:28:42.424338Z",
          "iopub.execute_input": "2025-09-16T19:28:42.42499Z",
          "iopub.status.idle": "2025-09-16T19:28:42.435946Z",
          "shell.execute_reply.started": "2025-09-16T19:28:42.424963Z",
          "shell.execute_reply": "2025-09-16T19:28:42.434949Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate percentage for each crop type\n",
        "\n",
        "### Subtask:\n",
        "Divide the total pixel count for each crop type by the total number of pixels across all images and multiply by 100 to get the percentage.\n"
      ],
      "metadata": {
        "id": "8fa05542"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Iterate through the crop_pixel_counts dictionary and calculate the percentage for each class by dividing by the total number of pixels across all images. Store the results in a new dictionary and print it.\n",
        "\n"
      ],
      "metadata": {
        "id": "f66abc4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a dictionary to store the percentages\n",
        "crop_percentages = {}\n",
        "\n",
        "# Iterate through the crop_pixel_counts dictionary\n",
        "for class_id, pixel_count in crop_pixel_counts.items():\n",
        "    # Calculate the percentage\n",
        "    percentage = (pixel_count / total_pixels_all_images) * 100\n",
        "    # Store the percentage in the new dictionary\n",
        "    crop_percentages[class_id] = percentage\n",
        "\n",
        "# Print the calculated percentages\n",
        "print(\"Percentage of pixels predicted for each crop type:\")\n",
        "print(crop_percentages)"
      ],
      "metadata": {
        "id": "347050f3",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:28:47.095996Z",
          "iopub.execute_input": "2025-09-16T19:28:47.096258Z",
          "iopub.status.idle": "2025-09-16T19:28:47.101273Z",
          "shell.execute_reply.started": "2025-09-16T19:28:47.09624Z",
          "shell.execute_reply": "2025-09-16T19:28:47.100351Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display the results\n",
        "\n",
        "### Subtask:\n",
        "Present the calculated percentages for each crop type.\n"
      ],
      "metadata": {
        "id": "06603008"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Define a dictionary mapping class IDs to crop names, print a header, and then iterate through the `crop_percentages` dictionary to print the percentage for each crop type using the defined mapping.\n",
        "\n"
      ],
      "metadata": {
        "id": "bec1808f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary mapping class IDs to crop type names\n",
        "class_id_to_crop = {\n",
        "    1: 'Gram',\n",
        "    2: 'Maize',\n",
        "    3: 'Mustard',\n",
        "    4: 'Sugarcane',\n",
        "    5: 'Wheat'\n",
        "}\n",
        "\n",
        "# Print a header\n",
        "print(\"Percentage of pixels predicted for each crop type:\")\n",
        "\n",
        "# Iterate through the crop_percentages dictionary and print the results\n",
        "for class_id, percentage in crop_percentages.items():\n",
        "    crop_name = class_id_to_crop.get(class_id, f\"Unknown Class {class_id}\")\n",
        "    print(f\"{crop_name}: {percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "c6189bf2",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-16T19:28:52.825838Z",
          "iopub.execute_input": "2025-09-16T19:28:52.826612Z",
          "iopub.status.idle": "2025-09-16T19:28:52.831604Z",
          "shell.execute_reply.started": "2025-09-16T19:28:52.826589Z",
          "shell.execute_reply": "2025-09-16T19:28:52.830654Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The total number of pixels across all images in the submission dataset is 8,257,536.\n",
        "*   Based on the provided `submission.csv` and the implemented RLE decoding logic, the total number of pixels predicted for each of the specified crop types (Gram, Maize, Mustard, Sugarcane, and Wheat) is 0.\n",
        "*   Consequently, the percentage of pixels predicted for Gram, Maize, Mustard, Sugarcane, and Wheat is 0.00% for each crop type.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the `EncodedPixels` column in the `submission.csv` file to understand why the RLE decoding resulted in zero predicted pixels for the specified crop types. This could involve checking for empty or invalid RLE strings.\n"
      ],
      "metadata": {
        "id": "a77e6b3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task\n",
        "Improve the semantic segmentation model to correctly predict crop types and increase accuracy, addressing the current output of all zeros for all crop types."
      ],
      "metadata": {
        "id": "7c572b46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review model architecture and parameters\n",
        "\n",
        "### Subtask:\n",
        "Examine the current model architecture, hyperparameters (like learning rate, optimizer, etc.), and loss function to ensure they are suitable for this semantic segmentation task and dataset.\n"
      ],
      "metadata": {
        "id": "be87908b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "Review the model architecture, hyperparameters, and loss function based on the provided code and identify potential areas for improvement.\n",
        "\n"
      ],
      "metadata": {
        "id": "5814f811"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Review of Model Configuration:\n",
        "\n",
        "# 1. Model Architecture:\n",
        "# Backbone: 'prithvi_eo_v2_300_tl' - This seems to be a large, potentially powerful pre-trained backbone.\n",
        "# Decoder: 'UperNetDecoder' - UPerNet is a common and effective decoder for semantic segmentation.\n",
        "# This combination is generally suitable for semantic segmentation of complex imagery like satellite data.\n",
        "# Potential improvement: While the architecture is good, its size might be an issue on limited hardware,\n",
        "# or it might require extensive fine-tuning.\n",
        "\n",
        "# 2. Hyperparameters and Loss Function:\n",
        "# Loss: 'dice' - Dice loss is good for handling class imbalance, which is common in segmentation.\n",
        "# Potential improvement: Combining Dice loss with Cross-Entropy loss can sometimes lead to better results.\n",
        "# This can help with both class imbalance and capturing boundary information.\n",
        "\n",
        "# Learning Rate (lr): 5e-7 - This is a very low learning rate. While it might prevent instability with a large pre-trained model,\n",
        "# it could also slow down convergence significantly, especially if the backbone is not fully frozen.\n",
        "# Potential improvement: A slightly higher learning rate might be beneficial, potentially with a learning rate scheduler.\n",
        "\n",
        "# Optimizer: 'AdamW' - AdamW is a good choice for many deep learning tasks, including segmentation.\n",
        "\n",
        "# freeze_backbone: False - The backbone is not frozen, which is good for fine-tuning on the specific dataset.\n",
        "# However, with a very low learning rate and a large model, training might still be slow.\n",
        "\n",
        "# ignore_index: -1 - This seems appropriate if -1 is used to denote pixels that should be ignored (e.g., NoData).\n",
        "\n",
        "# 3. Identified Potential Areas for Improvement:\n",
        "# - Consider combining Dice loss with Cross-Entropy loss.\n",
        "# - Experiment with a slightly higher learning rate, potentially with a scheduler.\n",
        "# - Given the observation of all zero predictions, the current training might not be converging effectively. This could be due to the low learning rate or other training configuration issues not directly related to the model architecture itself (e.g., insufficient training epochs, data loading problems, etc.). However, focusing on the model config as per the subtask: the low learning rate is a primary suspect for slow or ineffective learning.\n",
        "\n",
        "print(\"Review of the current model configuration completed.\")\n",
        "print(\"\\nPotential areas for improvement:\")\n",
        "print(\"- Consider combining Dice loss with Cross-Entropy loss.\")\n",
        "print(\"- Experiment with a slightly higher learning rate and potentially a learning rate scheduler.\")\n",
        "print(\"- Ensure the training runs for enough epochs for the model to learn effectively at the chosen learning rate.\")"
      ],
      "metadata": {
        "id": "344c31b2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reasoning**:\n",
        "The review identified potential improvements in the loss function and learning rate. The next step is to modify the `SemanticSegmentationTask` to use a combined loss function and adjust the learning rate based on the identified potential improvements, then instantiate the model with the updated configuration.\n",
        "\n"
      ],
      "metadata": {
        "id": "bcd6f944"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to get prediction - UPDATED to handle dictionary output\n",
        "def predict(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, imgs, masks, filenames = [], [], [], [] # Include filenames to store\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader: # Iterate over batches (which are dictionaries)\n",
        "            images = batch['image'].to(device) # Extract images from the dictionary\n",
        "            targets = batch['mask'].to(device)  # Extract masks from the dictionary\n",
        "            filenames.extend(batch['filename']) # Extend the filenames list\n",
        "\n",
        "            outputs = model(images)             # raw logits\n",
        "            predictions = torch.argmax(outputs, dim=1)  # convert to class indices\n",
        "\n",
        "            preds.append(predictions.cpu())\n",
        "            imgs.append(images.cpu())\n",
        "            masks.append(targets.cpu())\n",
        "\n",
        "    # Concatenate the lists of tensors into single tensors\n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    masks = torch.cat(masks, dim=0)\n",
        "\n",
        "    return imgs, masks, preds, filenames # Return filenames as well"
      ],
      "metadata": {
        "id": "47877744",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torchmetrics"
      ],
      "metadata": {
        "id": "e496a2c8",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}